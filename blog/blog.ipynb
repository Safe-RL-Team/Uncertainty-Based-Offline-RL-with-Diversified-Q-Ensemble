{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ce5283f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "date: today  # TODO: change to date of publication\n",
    "author:\n",
    "    - name: Julian Dralle\n",
    "      affiliation: TU Berlin\n",
    "    - name: Jonas Loos\n",
    "      affiliation: TU Berlin\n",
    "bibliography: references.bib\n",
    "abstract: |\n",
    "    Overestimation of out-of-distribution actions is a key problem in offline reinforcement learning which can pose serious safety risks. \n",
    "    In this blog we explore two state-of-the-art algorithms proposed by @an2021edac, SAC-N and EDAC.\n",
    "    They handle uncertainty by training multiple critics in a Soft Actor Critic approach and taking the most pessimistic q-value guess to avoid overestimation.\n",
    "jupyter: python3\n",
    "execute:\n",
    "  enabled: true\n",
    "  echo: false\n",
    "  output: true\n",
    "theme:\n",
    "  light: \n",
    "    - default\n",
    "    - custom.scss\n",
    "  dark:\n",
    "    - darkly\n",
    "    - custom.scss\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2ad2-0d2e-4e23-97d7-1874247f2886",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Why Offline-RL?\n",
    "\n",
    "Training of RL algorithms require **active interaction** with the environment. Training can become quite time-consuming and expensive. It can even be dangerous in safety-critical domains like driving or healthcare. A trial-and-error procedure is basically prohibited. We cannot as an example let an agent explore, make mistakes, and learn while treating patients in a hospital.\n",
    "That's what makes learning from pre-collected experience so relevant. And fortunately we have already in many domains existing large datasets.\n",
    "Offline-RL therefore aims to learn policies using only these pre-collected data without further interactions with the environment.\n",
    "\n",
    "![Online and Offline Reinforcement Learning^[Figure taken from @offline_learning]](figures/offline_rl.gif){#fig-orl fig-align=\"center\" width=80%}\n",
    "\n",
    "### What properties make offline-RL difficult?\n",
    "\n",
    "But offline RL comes with its own challenges. By far the biggest problem are so called **out of distribution (OOD) actions**. OOD actions refer to actions taken by an agent that fall outside the range of actions observed in the training dataset. \n",
    "State-action space can become so vast that the dataset cannot cover all of it. Especially narrow and biased datasets lack significant coverage and can lead to problems with OOD actions. For example, healthcare datasets are often biased towards serious cases. Only seriously ill people are getting treated, while healthier people are sent home untreated.\n",
    "\n",
    "![Example for out of distribution actions: Success, i.e. recovery, rates for sick patients taking medicine and healthy patients not taking medicine.^[Figure taken from @d4rl_post]](figures/ood_medicine.png){#fig-ood-medicine fig-align=\"center\" width=80%}\n",
    "\n",
    "A naive algorithm might now conclude that treatment causes death, since there were no fatalities in the untreated (= healthy) patients. Choosing to not treat a severely sick patient is something that never happened in the data, since the doctor would thereby violate his duty of care. Not treating a sick patient is therefore an OOD action. \n",
    "Vanilla RL algorithm might heavily overestimate the Q-values of OOD state-action pairs.\n",
    "\n",
    "### How to deal with OOD state-actions?\n",
    "\n",
    "\"Avoid OOD state-actions!\", has been the approach of many offline RL algorithms. This can be achieved by regularizing the policy to be close to the behavior policy that was used to collect the data. A more recent approach is to penalize the Q-values to be more pessimistic as done in Conservative Q-learning for Offline RL (CQL).\n",
    "But if we use this approach we require either (a) an estimation of the behavior policy or (b) explicit sampling from OOD data points (difficult!). Further, we prohibit our agent to approach any OOD state-actions, while some of these might actually be good. Q-function networks **do** have the ability to generalize. It's all about **handling the uncertainty** of these predictions. The agent might benefit from choosing some OOD data points which Q-values we can predict with high confidence.\n",
    "With SAC-N and EDAC An et al. (2021) found a way of **effectively quantifying the Q-value estimates** by an ensemble of Q-function networks. In this blog we will explore and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d15ad6-be20-4cdc-bea7-c22023923d4c",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "\n",
    "### Q-Learning\n",
    "Like in standard reinforcement learning we want to find a policy $\\pi(a | s)$ that maximizes the cumulative discounted reward $\\mathbb{E}_{s_t, a_t}[...]$. \n",
    "The model-free [Q-learning](https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c) algorithm is a very common approach to learn the Q-function $Q_{\\phi}(s,a)$ with-in a neural network.\n",
    "\n",
    "### Actor-critic method\n",
    "In the standard deep actor-critic approach we use two networks: (1) a policy-based actor network and (2) a value-based critic network.\n",
    "\n",
    "![Structure of deep actor-critic RL^[Figure taken from @app10124236]](figures/actor_critic.webp){fig-align=\"center\" #fig-actor-critic width=80%}\n",
    "\n",
    "The critic network minimizes the Bellman residual. Note: In offline RL transitions are sampled from a static dataset $D$\n",
    "\n",
    "<!-- ![](figures/critic_formula.png) -->\n",
    "\n",
    "$$J_q(Q_\\phi) := \\mathbb{E}_{(s,a,s') \\sim D} \\left[ \\left( Q_\\phi(s,a) - \\left ( r(s,a) + \\gamma\\ \\mathbb{E}_{a'\\sim\\pi_\\phi(\\cdot|s')}[Q_{\\phi'}(s',a')] \\right)\\right)^2 \\right]$$\n",
    "\n",
    "The actor network is updated in an alternating fashion to maximizes the expected Q-value.\n",
    "\n",
    "<!-- ![](figures/actor_formula.png) -->\n",
    "\n",
    "$$J_p(\\pi_\\phi) := \\mathbb{E}_{s\\sim D, a\\sim\\pi_\\phi(\\cdot|s)} \\left[ Q_\\phi(s,a) \\right]$$\n",
    "\n",
    "### Conservative Q-Learning\n",
    "\n",
    "As of 2021, Conservative Q-Learning [@Kumar2020ConservativeQF] is the state-of-the-art for offline RL. It uses a “simple Q-value regularizer” to prevent the overestimation of OOD actions. \n",
    "\n",
    "<!-- ![](figures/cql.png) -->\n",
    "\n",
    "$$\\min_\\phi J_q(Q_\\phi)+\\alpha(\\mathbb{E}_{s\\sim D, a\\sim\\mu(\\cdot|s)}[Q_\\phi(s,a)] - \\mathbb{E}_{(s,a)\\sim D}[Q_\\phi(s,a)])$$\n",
    "\n",
    "<!-- TODO: check if it's really \\mu and not \\pi_\\phi -->\n",
    "\n",
    "For each state, CQL computes a distribution over actions using a temperature parameter $\\alpha$ that controls the amount of exploration. The distribution is a mixture of the behavior policy and the current Q-function. The closer $\\alpha$ is to 1 the more conservative.\n",
    "\n",
    "CQL will be used as the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59c651ef-5fc8-4d69-83f4-3d9edb5f0a6b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "D4RL^[@d4rl_post] (Datasets for Deep Data-Driven Reinforcement Learning) is a collection of standardized benchmark datasets for offline RL algorithms. \n",
    "It includes a variety of environments but we focus on the MuJoCo Gym environments \"Half Cheetah\", \"Hopper\", and \"Walker2D\".\n",
    "\n",
    "::: {#fig-mujoco-gifs layout-ncol=3}\n",
    "![Half Cheetah](figures/half_cheetah.gif)\n",
    "\n",
    "![Hopper](figures/hopper.gif)\n",
    "\n",
    "![Walker2D](figures/walker2d.gif)\n",
    "\n",
    "Agents that take random actions in the different MuJoCo environments.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b160c1d4",
   "metadata": {},
   "source": [
    "```{=html}\n",
    "<details>\n",
    "<summary>Click to control a halfcheetah</summary>\n",
    "```\n",
    "\n",
    "Below you can interactively change the angles of the legs of the halfcheetah, similar to how the RL agents control them^[This is only a rough approximation to how the halfcheetah environment works. One of the main differences is that the agents control the torque on the joints instead of the angle.].\n",
    "\n",
    "```{ojs #fig-halfcheetah-slider}\n",
    "viewof angles = Inputs.form({\n",
    "    back1:  Inputs.range([-1,1], {value:0, step:0.1, label: \"angle back 1:\"}),\n",
    "    back2:  Inputs.range([-1,1], {value:0, step:0.1, label: \"angle back 2:\"}),\n",
    "    back3:  Inputs.range([-1,1], {value:0, step:0.1, label: \"angle back 3:\"}),\n",
    "    front1: Inputs.range([-1,1], {value:0, step:0.1, label: \"angle front 1:\"}),\n",
    "    front2: Inputs.range([-1,1], {value:0, step:0.1, label: \"angle front 2:\"}),\n",
    "    front3: Inputs.range([-1,1], {value:0, step:0.1, label: \"angle front 3:\"}),\n",
    "})\n",
    "\n",
    "leg_length = 0.4;\n",
    "foot_length = 0.2;\n",
    "\n",
    "back1 = angles.back1 + 0.6;\n",
    "back2 = back1 + angles.back2 - 1.3;\n",
    "back3 = back2 + angles.back3 + 1.6;\n",
    "front1 = angles.front1 - 0.4;\n",
    "front2 = front1 + angles.front2 + 0.8;\n",
    "front3 = front2 + angles.front3 + 0.3;\n",
    "\n",
    "joint_back1 = ({x: -0.5, y: 0.4});\n",
    "joint_back2 = ({x: joint_back1.x + Math.sin(back1) * 0.40, y: joint_back1.y - Math.cos(back1) * 0.40});\n",
    "joint_back3 = ({x: joint_back2.x + Math.sin(back2) * 0.40, y: joint_back2.y - Math.cos(back2) * 0.36});\n",
    "joint_back4 = ({x: joint_back3.x + Math.sin(back3) * 0.16, y: joint_back3.y - Math.cos(back3) * 0.18});\n",
    "joint_front1 = ({x: 0.5, y: 0.4});\n",
    "joint_front2 = ({x: joint_front1.x + Math.sin(front1) * 0.33, y: joint_front1.y - Math.cos(front1) * 0.3});\n",
    "joint_front3 = ({x: joint_front2.x + Math.sin(front2) * 0.30, y: joint_front2.y - Math.cos(front2) * 0.28});\n",
    "joint_front4 = ({x: joint_front3.x + Math.sin(front3) * 0.15, y: joint_front3.y - Math.cos(front3) * 0.16});\n",
    "\n",
    "line_conf = ({x: 'x', y: 'y', strokeWidth: 15});\n",
    "line_conf1 = ({...line_conf, stroke: '#91775b'});\n",
    "line_conf2 = ({...line_conf, stroke: '#956f6e'});\n",
    "Plot.plot({\n",
    "    marks: [\n",
    "        Plot.line([joint_back1, joint_front1], line_conf1),  // back\n",
    "        Plot.line([joint_front1, {x: joint_front1.x+0.2, y: joint_front1.y+0.2}], line_conf1),  // head\n",
    "        Plot.line([joint_back1, joint_back2], line_conf1),  // back leg 1\n",
    "        Plot.line([joint_back2, joint_back3], line_conf2),  // back leg 2\n",
    "        Plot.line([joint_back3, joint_back4], line_conf2),  // back foot\n",
    "        Plot.line([joint_front1, joint_front2], line_conf1),  // front leg 1\n",
    "        Plot.line([joint_front2, joint_front3], line_conf2),  // front leg 2\n",
    "        Plot.line([joint_front3, joint_front4], line_conf2),  // front foot\n",
    "    ],\n",
    "    width: 600,\n",
    "    height: 400,\n",
    "    x: {\n",
    "        domain: [-1,1],\n",
    "        axis: null,\n",
    "    },\n",
    "    y: {\n",
    "        domain: [-2/3,2/3],\n",
    "        axis: null,\n",
    "    },\n",
    "});\n",
    "\n",
    "```\n",
    "\n",
    "```{=html}\n",
    "</details>\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d2d5d37",
   "metadata": {},
   "source": [
    "MuJoCo stands for Multi-joint dynamics with Contact. It is a fast and accurate physics simulation engine for robotics, biomechanics, and others. \n",
    "The environments are stochastic in terms of their initial state, with a Gaussian noise added to a fixed initial state in order to add stochasticity. \n",
    "Goal in the gym tasks is, to run as fast as possible to the right by applying a torque on the joints. A negative reward is allocated for moving backwards.\n",
    "The observation space consists of body/joint position and velocity, while an action represents the torques applied between links.\n",
    "\n",
    "As an example, the half cheetah environment has 6 joints and therefore a 6 dimensional action space which can be controlled using the inputs from a 17 dimensional observation space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a3b14-01cb-4ad2-ad2d-00a15e420003",
   "metadata": {},
   "source": [
    "The data is collected by training an SAC algorithm online until it reaches a certain performance level (medium or expert). Then this strategy is used to collect 1 millionen samples of data. In the full-replay dataset the training data is collected as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b9aa",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic (SAC-N)\n",
    "\n",
    "![Structure of deep actor-critic RL with multiple critics, as in SAC-N^[Figure taken from @app10124236 (modified)]](figures/actor_critic_sacn.png){fig-align=\"center\" #fig-actor-critic-sacn width=80%}\n",
    "\n",
    "The paper introduces two new methods for offline RL. The first method is called SAC-N and is an extension of Soft Actor-Critic (SAC) [@Haarnoja2018SoftAO], which is a popular off-policy actor-critic deep RL algorithm.\n",
    "SAC-N extends SAC by using the q-value of N instead of two q-functions, i.e. critics, as visualized in #fig-actor-critic-sacn. The q-values are then reduces to a single value by taking the minimum. The idea behind taking the minimum of more critics is that the resulting q-value is more pessimistic when the uncertainty is high. This prevents erroneously high q-values of OOD actions and therefore trains the actor to prefer safer actions.\n",
    "\n",
    "The minimum of multiple critics approximates the true q-value minus a multiple of the standard deviation [@an2021edac]:\n",
    "\n",
    "$$ \\mathbb{E}\\left [\\min_{i=1,...,N}Q_i\\right] \\approx m - \\Phi^{-1}\\left(\\frac{N-\\pi/8}{N-\\pi/4+1}\\right) \\sigma $$\n",
    "\n",
    "Where $N$ is the number of critics, $Q_i$ is the q-value of the $i$-th critic, $m$ is the theoretical true q-value, $\\Phi$ is the CDF of the standard gaussian distribution, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "This is visualized in the diagram below, where q-value estimates over an exemplary action space are plotted.The black line is the theoretical true q-value and the grey area its standard deviation. The lightblue lines represent the critics, that try to approximate the true q-value. The bottom blue line is the minimum of the critics, that should, especially for a high number of critics, be roughly the true q-value minus a multiple of the standard deviation. You can use the slider to change the number of critics:\n",
    "\n",
    "```{ojs #fig-critics-slider}\n",
    "means = [0,2,-1,0,1,1,2,0,-1,-2,0,1,2,4,3,3,1,0,-1,0];\n",
    "stds  = [2,3, 2,1,2,3,2,3, 4, 2,3,4,2,1,3,2,1,2, 3,2];\n",
    "viewof num_critics = Inputs.range([1,50], {value:20, step:1, label: \"#critics: \"});\n",
    "function gaussianRandom(mean=0, stdev=1) {\n",
    "    let u = 1 - Math.random(); // Converting [0,1) to (0,1]\n",
    "    let v = Math.random();\n",
    "    let z = Math.sqrt(-2.0*Math.log(u)) * Math.cos(2.0*Math.PI*v);\n",
    "    return z * stdev + mean;\n",
    "}\n",
    "function example_critic() {\n",
    "  return [...Array(means.length).keys()].map((i)=>gaussianRandom(means[i], stds[i]));\n",
    "}\n",
    "function toPlot(data, color){\n",
    "  return Plot.line(data.map((x, i)=>({\"action space\": i, \"q-value\": x})), {x: \"action space\", y: \"q-value\", stroke: color});\n",
    "}\n",
    "critics = [...Array(num_critics).keys()].map(i=>example_critic());\n",
    "Plot.plot({\n",
    "  marks: [\n",
    "      Plot.areaY(means.map((x,i)=>({\"action space\": i, low:x-stds[i], high:x+stds[i]})), {x: \"action space\", y1: \"low\", y2: \"high\", fill: \"#ddd\"}),\n",
    "      ...(critics.map(x=>toPlot(x, \"lightblue\"))),\n",
    "      toPlot([...Array(means.length).keys()].map(i=>Math.min(...critics.map(x=>x[i]))), \"blue\"),\n",
    "      toPlot(means, \"black\"),\n",
    "  ],\n",
    "  y: {\n",
    "    domain: [-10,10],\n",
    "    label: \"q-value\",\n",
    "  },\n",
    "  x: {\n",
    "    tickFormat: x => \"\",\n",
    "  },\n",
    "  width: 796,\n",
    "});\n",
    "```\n",
    "\n",
    "SAC-N already achieves notable performance and beats the previous state of the art, CQL, as will be shown in the [results section](#results). However, SAC-N requires a large number of critics, which comes with a high computational cost. Therefore, the paper introduces a second method, EDAC, that is more efficient.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "682613d3",
   "metadata": {},
   "source": [
    "## Ensemble-Diversified Actor Critic (EDAC)\n",
    "\n",
    "@an2021edac found, that the performance of the policy learned by SAC-N decreases significantly, when the q-functions share a similar local structure. To reduce this, they introduce an ensemble gradient diversification term to the loss function of the ensemble of critics:\n",
    "\n",
    "$$ \\underset\\phi{\\text{minimize}}\\ \\ \\frac{1}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle $$\n",
    "\n",
    "It measures the cosine similarity between the q-function gradients and is minimized when the gradients for the critics are as different as possible. This, in turn, leads to a more diverse ensemble of critics, which is more robust against overestimation of OOD actions.\n",
    "\n",
    "![Illustration of the ensemble gradient diversification. The vector $\\lambda_iw_i$ represents the normalized eigenvector $w_i$ of $\\text{Var}(\\nabla_a Q_{\\phi_j}(s,a))$ multiplied by its eigenvalue $\\lambda_i$. ^[Figure taken from @an2021edac]](figures/EDAC_gradient_diversification.png){#fig-EDAC}\n",
    "\n",
    "The full loss function of the critics is then:\n",
    "\n",
    "\n",
    "$$\\nabla_{\\phi_i} \\frac{1}{|B|} \\sum_{(s,a,r,s')\\in B} \\left (\\left( Q_{\\phi_i}(s,a) - y(r, s') \\right)^2 + \\frac{\\eta}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle \\right)$$\n",
    "\n",
    "where $B$ is the batch of transitions, $y(r, s')$ is the target q-function^[Using target critics / q-functions can help to reduce instabilities during training. For more details, you can read the code under [Implementation](#implementation)], and $\\eta$ is the hyperparameter for how much the ensemble gradient diversification term should be weighted.\n",
    "\n",
    "Note that EDAC reduces to SAC-N when $\\eta=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ff798",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "There are multiple implementations of EDAC and SAC-N available.\n",
    "@an2021edac published their implementation on [GitHub](https://github.com/snu-mllab/EDAC). It contains 9712 lines of python code over 93 files.\n",
    "\n",
    "Another implementation is part of the Clean Offline Reinforcement Learning (CORL) [Repository](https://github.com/tinkoff-ai/CORL), which aims to provide single-file implementations of SOTA offline RL algorithms. Its EDAC implementation contains 639 lines of code in a single file. This makes it significantly easier to understand and modify. We therefore used it for some of our experiments and as a inspiration for our own implementation.^[Our fork of CORL for our experiments is also available on [GitHub](https://github.com/JonasLoos/CORL)]\n",
    "\n",
    "We also [implemented EDAC](https://github.com/JonasLoos/edac_reimplementation) from scratch in [PyTorch](https://pytorch.org/) and managed to achieve a code size of 379 lines, while adding additional features^[We added features like the continuation of training runs, and new critic ensemble reduction functions (instead of min) to our implementation.]. Our [results](#results) below are based on our implementation until stated otherwise.\n",
    "\n",
    "### Code\n",
    "\n",
    "A simplified version of the main parts of our [`train`](https://github.com/JonasLoos/edac_reimplementation/blob/main/edac.py#L152) function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1650742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`def train(config, ...):`\"\n",
    "# initialize environment, and set seeds\n",
    "...\n",
    "\n",
    "# initialize models\n",
    "actor = Actor([state_dim, 256, 256, action_dim], ...)\n",
    "critic = VectorCritic([state_dim + action_dim, 256, 256, 1], ...)\n",
    "target_critic = deepcopy(critic)\n",
    "log_beta = torch.tensor(0.0, requires_grad=True)\n",
    "beta = log_beta.exp()\n",
    "\n",
    "# initialize optimizers (Adam)\n",
    "...\n",
    "\n",
    "# set critic ensemble reduction function, by default `min`\n",
    "...\n",
    "\n",
    "# load checkpoint if given, save the config, and initialize logging\n",
    "...\n",
    "\n",
    "# main training loop\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    for step in range(config.updates_per_epoch):\n",
    "        # sample batch of transitions\n",
    "        state, action, reward, next_state, done = buffer.sample()\n",
    "\n",
    "        # calculate q-target\n",
    "        next_action, log_prob = actor(next_state)\n",
    "        q_next = (\n",
    "            critic_reduction(target_critic(next_state, next_action))\n",
    "             - beta * log_prob\n",
    "        )\n",
    "        q_target = reward + config.gamma * (1 - done) * q_next\n",
    "\n",
    "        # update critics\n",
    "        base_critic_loss = (critic(state, action) - q_target).pow(2)\n",
    "        q_gradients = torch.autograd.grad(critic(...), ..., create_graph=True)\n",
    "        diversity_loss = (q_gradients @ q_gradients.T) * (1-torch.eye(N)) / (N-1)\n",
    "        critic_loss = base_critic_loss.sum(-1) + config.eta * diversity_loss.sum(1,2)\n",
    "        ...\n",
    "\n",
    "        # update beta\n",
    "        actor_action, actor_action_log_prob = actor(state)\n",
    "        beta_loss = (-log_beta * (actor_action_log_prob - action_dim))\n",
    "        ...\n",
    "        beta = log_beta.exp()\n",
    "\n",
    "        # update actor\n",
    "        actor_q_values = critic(state, actor_action)\n",
    "        actor_loss = -(critic_reduction(actor_q_values) - beta * actor_action_log_prob)\n",
    "        ...\n",
    "\n",
    "        # update target critic\n",
    "        for target_param, source_param in zip(\n",
    "                target_critic.parameters(), critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                (1 - config.tau) * target_param.data\n",
    "                 + config.tau * source_param.data\n",
    "            )\n",
    "\n",
    "    # save checkpoint, and log metrics\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3fa7d",
   "metadata": {},
   "source": [
    "Which uses separate classes for the `Actor` and `VectorCritic`^[The `VectorCritic` is a wrapper around a list of critics, which simplifies the handling of multiple critics.]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`class Actor(nn.Module):`\"\n",
    "def __init__(self, layer_sizes : list[int], ...):\n",
    "    ...\n",
    "    # setup hidden layers based on the given layer sizes\n",
    "    self.hidden = nn.Sequential(*(\n",
    "        x for i in range(len(layer_sizes) - 2) for x in [\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i + 1]),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "    ))\n",
    "    # create output and output uncertainty layers\n",
    "    self.output = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "    self.output_uncertainty = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "    # init parameters as in the EDAC paper\n",
    "    ...\n",
    "\n",
    "def forward(self, state):\n",
    "    x_hidden = self.hidden(state)\n",
    "    x_mean = self.output(x_hidden)\n",
    "    x_std = torch.exp(torch.clip(self.output_uncertainty(x_hidden), -5, 2))\n",
    "    policy_dist = Normal(x_mean, x_std)\n",
    "    action_linear = policy_dist.rsample()\n",
    "    action = torch.tanh(action_linear) * self.max_action\n",
    "    action_log_prob = policy_dist.log_prob(action_linear).sum(-1)\n",
    "    return action, action_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45182bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`VectorCritic(nn.Module):`\"\n",
    "def __init__(self, layer_sizes: list[int], num_critics: int):\n",
    "    ...\n",
    "    # create multiple critics with the architecture given by layer_sizes\n",
    "    # the output layer has no activation function\n",
    "    self.models = nn.ModuleList([\n",
    "        nn.Sequential(*[\n",
    "            x for i in range(len(layer_sizes) - 1) for x in [\n",
    "                nn.Linear(layer_sizes[i], layer_sizes[i + 1]),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "        ][:-1]) for _ in range(num_critics)\n",
    "    ])\n",
    "    # init parameters as in the EDAC paper\n",
    "    ...\n",
    "\n",
    "def forward(self, state, action):\n",
    "    return torch.cat([\n",
    "        model(torch.cat([state, action], dim=-1)) for model in self.models\n",
    "    ], dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7576a7f6",
   "metadata": {},
   "source": [
    "We used [pyrallis](https://github.com/eladrich/pyrallis) for the configuration and command line interface, and [Weights & Biases](https://wandb.ai/site) for tracking our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b69596",
   "metadata": {},
   "source": [
    "### Modifications\n",
    "\n",
    "We made a few modifications to the EDAC algorithm described in the paper.\n",
    "\n",
    "#### Dynamic $\\beta$\n",
    "\n",
    "As can be seen in the `train` function above, $\\beta$ (`beta`) is learned dynamically during training. While this is not explicitly mentioned in the pseudocode in the paper, the official implementation uses it, if `use_automatic_entropy_tuning` is set to `True`. As it can significantly improve training speed and the performance of the actor (at least for a limited training time), we decided to use it as well.\n",
    "\n",
    "However, we found that sometimes $\\beta$ would decrease continuously, which also limits the performance of the actor. How to prevent this could be a topic for future research.\n",
    "\n",
    "#### Alternative Critic Ensemble Reduction\n",
    "\n",
    "SAC-N, and therefore also EDAC, use the minimum q-value estimate of their critic ensembles. However, it seems like this could be too pessimistic in some cases, as q-value estimates cannot only be erroneously high, but also erroneously low. We tried to get better q-value estimations by subtracting a multiple of the standard deviation $\\sigma$ from the mean $\\mu$, i.e. $Q_{\\text{final}} = \\mu - \\alpha * \\sigma$, where $\\alpha$ is a hyperparameter^[In our implementation this can be set with e.g. `--critic_reduction=mean-4.2` for $\\alpha=4.2$]. The idea is to be more pessimistic, the higher the standard deviation is. Actually, as already mentioned in the section about [SAC-N](#soft-actor-critic-sac-n), this should be the expected value of the minimum q-value estimate for some specific $\\alpha$ and therefore be similar. However, as can be seen below in @fig-critic_reduction, it performs significantly worse than the minimum.\n",
    "\n",
    "Alternative critic ensemble reduction functions could be based on the median or use a weighted average of the q-values, where the weights decay exponentially for higher q-values. This, too, could be a topic for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3bee1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504ca5a-4c70-4c92-b204-3c3d289778b1",
   "metadata": {},
   "source": [
    "::: {#fig-batch-sizes layout=\"[[71,29]]\"}\n",
    "![(a) Average return of different batch sizes](figures/learning_batch_size.png)\n",
    "\n",
    "![(b) time in seconds to complete training](figures/batch_time.png)\n",
    "\n",
    "The learning curve for different batch sizes (a) and the time it takes for each batch size to conclude the full 150 epochs (b). These experiments were run using the CORL implementation on a NVIDIA GeForce RTX 3090.\n",
    ":::\n",
    "\n",
    "The learning curve is similar for all batch sizes. With n=2048 we have optimal performance.\n",
    "Note: In the paper they had batch size n=256 although they used the same graphics card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# imports for plotting the results\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbccdcf-bfaf-4c10-b10d-f1c99765a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-datasets\n",
    "#| fig-cap: \"Different Training Datasets\"\n",
    "\n",
    "df2 = pd.read_csv('data/datasets2.csv', delimiter=\";\")\n",
    "df2 = df2[0:400]\n",
    "\n",
    "df2.columns = ['epoch', 'full-replay', 'expert', 'medium-v2']\n",
    "\n",
    "x=df2['epoch']\n",
    "y=df2[['full-replay', 'expert', 'medium-v2']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 3):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Full-Replay best dataset, Expert doesn\\'t train',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Dataset\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c8f0-9149-4a8e-acdc-b83becc88f5f",
   "metadata": {},
   "source": [
    "Interpretation: Our implementation performs best on the full-replay dataset. From mistakes made during training the model can learn. The full replay has the best mix between these errors and good actions. This way there are less OOD actions.\n",
    "Problem: The model should be able to learn from biased data, especially from expert data. It is possible though, that training with the expert dataset just needs much more time to kickstart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5f7f6-e04d-4dd4-a45a-4e2add20e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-num_critics\n",
    "#| fig-cap: \"Number of Critics\"\n",
    "\n",
    "df3 = pd.read_csv('data/num_critics.csv', delimiter=\";\")\n",
    "df3 = df3[0:400]\n",
    "\n",
    "df3.columns = ['epoch', 'EDAC-10', 'EDAC-50', 'EDAC-20', 'EDAC-2', 'EDAC-5', 'SAC-20', 'SAC-10']\n",
    "\n",
    "x=df3['epoch']\n",
    "y=df3[['SAC-10','SAC-20','EDAC-2', 'EDAC-5', 'EDAC-10', 'EDAC-20', 'EDAC-50']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 7):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='EDAC-10 outperforms SAC-20',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Dataset\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ade09-4649-48d6-a1a2-5c64c437865c",
   "metadata": {},
   "source": [
    "Interpretation: EDAC with only 10 critics can outperform SAC-20. Increasing $N$ did not always improve the results for EDAC.\n",
    "The training duration is heavily dependant on the amount of critics.\n",
    "\n",
    "![Median time per epoch in seconds on the halfcheetah-full-replay dataset. This includes the training time with 500 steps and a batch size of 2048, and the testing time with 5 evaluation episodes. The experiments were run on a NVIDIA GeForce RTX 3090.](figures/time_per_epoch.svg){#fig-epoch-time fig-align=\"center\" width=80%}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc24d9-e847-4ff3-bd24-ed08d3227c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: fig-critic_reduction\n",
    "#| fig-cap: \"Average return of different critic reduction methods\"\n",
    "\n",
    "df = pd.read_csv('data/critic_reduction.csv', delimiter=\",\")\n",
    "\n",
    "df.columns = ['epoch', 'mean-8', 'mean-4', 'mean-2', 'mean-1', 'mean-0.5', 'mean', 'minimum']\n",
    "\n",
    "\n",
    "x=df['epoch']\n",
    "y=df[['mean-4', 'mean-2', 'mean-1', 'mean-0.5', 'mean', 'minimum']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 6):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Custom Critic Reduction worse than Minimum',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Model\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056215a-8038-47c0-b884-2cc7b9e13340",
   "metadata": {},
   "source": [
    "Interpretation: Using a custom critic reduction instead of the minimum, did not work with EDAC. The standard deviation of the ensemble might fluctuate too much to be \"contained\" by a static parameter.\n",
    "Other approaches could be to try out this method with SAC-N. Another idea is to take the median instead of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6899e83-a715-49b0-90df-ec7101437146",
   "metadata": {},
   "source": [
    "### Videos\n",
    "\n",
    "\n",
    "::: {layout=\"[[-10,60,-10],[49,-2,49]]\"}\n",
    "\n",
    "::: {#fig-vid-halfcheetah}\n",
    "{{< video figures/EDAC_reimplementation_halfcheetah.mp4 >}}\n",
    "<!-- {{< video figures/EDAC_reimplementation_halfcheetah-full-replay-230319-031314.mp4 width=\"400px\" >}} -->\n",
    "This video shows an agent trained on the halfcheetah task.\n",
    ":::\n",
    "\n",
    "::: {#fig-vid-walker2d}\n",
    "{{< video figures/EDAC_reimplementation_walker2d-full-replay-230319-164301.mp4 >}}\n",
    "This video shows an agent trained on the walker2d task.\n",
    ":::\n",
    "\n",
    "::: {#fig-vid-walker2d}\n",
    "{{< video figures/EDAC_reimplementation_hopper-medium-230322-124459.mp4 >}}\n",
    "This video shows an agent trained on the hopper task.\n",
    ":::\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c467aef",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4b21d",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "This blog post was created for the Advanced Topics in Reinforcement Learning seminar (2022/23) at TU Berlin. We would like to thank Dr. Rong Guo for supervising this seminar.\n",
    "\n",
    "The code for this blog is available on [GitHub](https://github.com/Safe-RL-Team/Uncertainty-Based-Offline-RL-with-Diversified-Q-Ensemble)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

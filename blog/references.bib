@inproceedings{an2021edac,
    title={Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble},
    author={Gaon An and Seungyong Moon and Jang-Hyun Kim and Hyun Oh Song},
    booktitle={Neural Information Processing Systems},
    url={https://arxiv.org/pdf/2110.01548.pdf},
    year={2021}
}

@inproceedings{Haarnoja2018SoftAO,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Tuomas Haarnoja and Aurick Zhou and P. Abbeel and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/pdf/1801.01290.pdf},
  year={2018}
}

@article{Kumar2020ConservativeQF,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Aviral Kumar and Aurick Zhou and G. Tucker and Sergey Levine},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.04779},
  url={https://arxiv.org/pdf/2006.04779.pdf}
}

@Article{app10124236,
    AUTHOR = {Giang, Hoang Thi Huong and Hoan, Tran Nhut Khai and Thanh, Pham Duy and Koo, Insoo},
    TITLE = {Hybrid NOMA/OMA-Based Dynamic Power Allocation Scheme Using Deep Reinforcement Learning in 5G Networks},
    JOURNAL = {Applied Sciences},
    VOLUME = {10},
    YEAR = {2020},
    NUMBER = {12},
    ARTICLE-NUMBER = {4236},
    URL = {https://www.mdpi.com/2076-3417/10/12/4236},
    ISSN = {2076-3417},
    ABSTRACT = {Non-orthogonal multiple access (NOMA) is considered a potential technique in fifth-generation (5G). Nevertheless, it is relatively complex when applying NOMA to a massive access scenario. Thus, in this paper, a hybrid NOMA/OMA scheme is considered for uplink wireless transmission systems where multiple cognitive users (CUs) can simultaneously transmit their data to a cognitive base station (CBS). We adopt a user-pairing algorithm in which the CUs are grouped into multiple pairs, and each group is assigned to an orthogonal sub-channel such that each user in a pair applies NOMA to transmit data to the CBS without causing interference with other groups. Subsequently, the signal transmitted by the CUs of each NOMA group can be independently retrieved by using successive interference cancellation (SIC). The CUs are assumed to harvest solar energy to maintain operations. Moreover, joint power and bandwidth allocation is taken into account at the CBS to optimize energy and spectrum efficiency in order to obtain the maximum long-term data rate for the system. To this end, we propose a deep actor-critic reinforcement learning (DACRL) algorithm to respectively model the policy function and value function for the actor and critic of the agent (i.e., the CBS), in which the actor can learn about system dynamics by interacting with the environment. Meanwhile, the critic can evaluate the action taken such that the CBS can optimally assign power and bandwidth to the CUs when the training phase finishes. Numerical results validate the superior performance of the proposed scheme, compared with other conventional schemes.},
    DOI = {10.3390/app10124236}
}

@article{d4rl_post,
  author = {Justin Fu},
  title = {D4RL: Building Better Benchmarks for Offline Reinforcement Learning},
  journal = {Berkeley Artificial Intelligence Research},
  year = {2020},
  url = {https://bair.berkeley.edu/blog/2020/06/25/D4RL/}
}

@misc{offline_learning, 
  title={An optimistic perspective on offline reinforcement learning}, 
  url={https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html}, 
  journal={Google AI Blog}, 
  author={Rishabh Agarwal and Mohammad Norouzi}
} 